{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Vision: Interactive Attention & Patch Updates\n",
    "\n",
    "Use a slider to scrub through an image sequence. For each frame, this notebook:\n",
    "- Runs differential vision encoding (cache / partial / full) using `DifferentialVisionEncoder`.\n",
    "- Computes patch-change overlays and token delta maps.\n",
    "- Captures text→vision attention and overlays it on the patch grid.\n",
    "- Exposes controls for diff threshold, max changed patches, context radius, attention head/layer, and query offset.\n",
    "\n",
    "Instructions:\n",
    "- Set `model_path` and `images_dir` in the config block below.\n",
    "- Run the cell to initialize the UI and use the controls to explore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262d3548",
   "metadata": {},
   "outputs": [],
   "source": "# Interactive differential vision + attention over a sequence\n\nimport os, time, math\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport ipywidgets as W\nfrom IPython.display import display, clear_output\n\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import tokenizer_image_token, process_images\nfrom llava.constants import DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom llava.conversation import conv_templates\nfrom llava.utils import disable_torch_init\n\nfrom differential_vision import DifferentialVisionEncoder\nfrom differential_vision.patch_utils import (\n    compute_patch_diff, compute_patch_diff_values,\n    visualize_changed_patches, overlay_patch_rects\n)\n\n# ---------- Config ----------\ndevice = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\ntorch_device = torch.device(device)\n\n# Edit these before running the cell\nmodel_path = '../checkpoints/llava-fastvithd_1.5b_stage3'\nmodel_base = None  # set base if your checkpoint is projector-only; else keep None\nimages_dir = '../differential_vision/agentnet_curated/traj_0000'\nprompt = 'Describe salient changes and where the text points its focus.'\nforce_pad_aspect = True  # keeps single-image path for differential updates\n\n# ---------- Load model ----------\ndisable_torch_init()\nmodel_name = os.path.basename(os.path.expanduser(model_path))\ntokenizer, model, image_processor, _ = load_pretrained_model(\n    os.path.expanduser(model_path), model_base, model_name, device=device\n)\nif force_pad_aspect:\n    try: setattr(model.config, 'image_aspect_ratio', 'pad')\n    except Exception: pass\n\n# ---------- Build prompt/token ids (once) ----------\ndef build_prompt_and_ids(tokenizer, model, user_prompt):\n    qs = user_prompt\n    if getattr(model.config, 'mm_use_im_start_end', False):\n        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n    else:\n        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n    conv = conv_templates['qwen_2'].copy()\n    conv.append_message(conv.roles[0], qs)\n    conv.append_message(conv.roles[1], None)\n    prompt_text = conv.get_prompt()\n    input_ids = tokenizer_image_token(\n        prompt_text, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt'\n    ).unsqueeze(0).to(torch_device)\n    return prompt_text, input_ids\n\nprompt_text, input_ids = build_prompt_and_ids(tokenizer, model, prompt)\n\n# ---------- Prepare frames ----------\ndef _natural_key(path: str):\n    import re\n    base = os.path.basename(path)\n    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', base)]\n\ndef gather_images(directory: str):\n    supported = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n    files = [os.path.join(directory, f) for f in os.listdir(directory)\n             if os.path.splitext(f)[1].lower() in supported]\n    return sorted(files, key=_natural_key)\n\nframe_paths = gather_images(os.path.expanduser(images_dir))\nassert frame_paths, f'No images found in {images_dir}'\n\ndef load_and_process(path):\n    pil = Image.open(path).convert('RGB')\n    processed = process_images([pil], image_processor, model.config)\n    image_tensor = processed[0] if isinstance(processed, (list, tuple)) else processed\n    if image_tensor.ndim == 3:\n        image_tensor = image_tensor.unsqueeze(0)\n    return pil, image_tensor.to(device=torch_device, dtype=next(model.parameters()).dtype)\n\nframes_pil = []\nframes_tensor = []\nfor p in frame_paths:\n    pil, ten = load_and_process(p)\n    frames_pil.append(pil)\n    frames_tensor.append(ten)\n\n# ---------- Vision grid ----------\nvt = model.get_vision_tower()\ngrid_side = getattr(vt, 'num_patches_per_side', None)\nif grid_side is None:\n    # fallback to square root of token count\n    with torch.inference_mode():\n        tmp_feats = model.encode_images(frames_tensor[0])\n    grid_side = int(round(math.sqrt(int(tmp_feats.shape[1]))))\ngrid_side = int(grid_side)\n\nif hasattr(vt, 'config') and hasattr(vt.config, 'patch_size'):\n    patch_px_default = int(vt.config.patch_size)\nelif isinstance(getattr(vt, 'config', None), dict):\n    patch_px_default = int(vt.config['image_cfg']['patch_size'])\nelse:\n    patch_px_default = 24\n\n# ---------- Widgets ----------\nframe_slider = W.IntSlider(description='Frame', min=0, max=len(frame_paths)-1, step=1, value=0, continuous_update=False)\ndiff_thr = W.FloatSlider(description='Diff thr', min=0.0, max=1.0, step=0.01, value=0.05, readout_format='.2f', continuous_update=False)\nmax_changed = W.IntSlider(description='Max Δ patches', min=1, max=grid_side*grid_side, step=1, value=50, continuous_update=False)\nctx_radius = W.IntSlider(description='Context r', min=0, max=3, step=1, value=1, continuous_update=False)\n\natt_layer_mode = W.Dropdown(description='Layer', options=['last', 'mean_all'], value='last')\natt_head_mode = W.Dropdown(description='Head', options=['mean', 'index'], value='mean')\natt_head_idx = W.IntSlider(description='Head idx', min=0, max=15, step=1, value=0, continuous_update=False)\nq_offset = W.IntSlider(description='Q offset (from end)', min=0, max=8, step=1, value=0, continuous_update=False)\n\nreset_seq_btn = W.Button(description='Reset sequence state', button_style='warning')\noutput_widget = W.Output()\n\nui_top = W.HBox([frame_slider, diff_thr, max_changed, ctx_radius])\nui_attn = W.HBox([att_layer_mode, att_head_mode, att_head_idx, q_offset])\nui_ctrl = W.HBox([reset_seq_btn])\n\ndisplay(ui_top, ui_attn, ui_ctrl, output_widget)\n\n# ---------- State ----------\nstate = dict(\n    encoder=None,\n    feats_prev=None,\n    last_idx=None,\n    vt=vt\n)\n\ndef build_encoder():\n    return DifferentialVisionEncoder(\n        model,\n        diff_threshold=float(diff_thr.value),\n        max_changed_patches=int(max_changed.value),\n        context_radius=int(ctx_radius.value),\n        skip_small_updates=False,\n        device=torch_device\n    )\n\ndef reset_sequence():\n    state['encoder'] = build_encoder()\n    state['encoder'].reset_cache()\n    state['feats_prev'] = None\n    state['last_idx'] = None\n\nreset_sequence()\n\n# ---------- Attention helpers ----------\ndef get_vision_span(input_ids, image_features):\n    N = int(image_features.shape[1])\n    pos = (input_ids[0] == IMAGE_TOKEN_INDEX).nonzero(as_tuple=False)\n    assert pos.numel() > 0, 'Prompt must contain an IMAGE_TOKEN sentinel'\n    vision_start = int(pos[0, 0].item())\n    return vision_start, vision_start + N, N\n\ndef fuse_attention(attentions, q_index, v_start, v_end, layer_mode='last', head_mode='mean', head_idx=0):\n    if layer_mode == 'last':\n        L_sel = [attentions[-1]]\n    else:\n        L_sel = attentions\n    layer_maps = []\n    for A in L_sel:\n        A = A[0]                  # [H,Q,K]\n        A_q = A[:, q_index, :]    # [H,K]\n        if head_mode == 'index':\n            A_q = A_q[head_idx:head_idx+1, :].mean(dim=0)\n        else:\n            A_q = A_q.mean(dim=0)\n        layer_maps.append(A_q)\n    A_mean = torch.stack(layer_maps, dim=0).mean(dim=0)  # [K]\n    return A_mean[v_start:v_end]\n\n# ---------- Rendering ----------\ndef replay_to_index(idx):\n    reset_sequence()\n    feats_prev = None\n    for t in range(0, idx+1):\n        feats_t, _ = state['encoder'].encode(frames_tensor[t], return_stats=True)\n        feats_prev = feats_t\n    state['feats_prev'] = feats_prev\n    state['last_idx'] = idx\n\ndef render(idx):\n    if (state['encoder'] is None or\n        state['encoder'].diff_threshold != float(diff_thr.value) or\n        state['encoder'].max_changed_patches != int(max_changed.value) or\n        state['encoder'].context_radius != int(ctx_radius.value)):\n        replay_to_index(idx)\n    else:\n        if state['last_idx'] is None or idx < state['last_idx']:\n            replay_to_index(idx)\n        elif idx > state['last_idx']:\n            for t in range(state['last_idx']+1, idx+1):\n                feats_t, _ = state['encoder'].encode(frames_tensor[t], return_stats=True)\n                state['feats_prev'] = feats_t\n            state['last_idx'] = idx\n\n    feats_cur, info = state['encoder'].encode(frames_tensor[idx], return_stats=True)\n\n    vt = state['vt']\n    grid_side_local = getattr(vt, 'num_patches_per_side', grid_side)\n    grid_side_local = int(grid_side_local)\n    if hasattr(vt, 'config') and hasattr(vt.config, 'patch_size'):\n        patch_px = int(vt.config.patch_size)\n    elif isinstance(getattr(vt, 'config', None), dict):\n        patch_px = int(vt.config['image_cfg']['patch_size'])\n    else:\n        patch_px = patch_px_default\n\n    img_cur = np.array(frames_pil[idx]).astype(np.float32)/255.0\n    if idx > 0:\n        img_prev = np.array(frames_pil[idx-1]).astype(np.float32)/255.0\n        mask_bool = compute_patch_diff(img_prev, img_cur, patch_size=patch_px, threshold=float(diff_thr.value))\n        diff_vals = compute_patch_diff_values(img_prev, img_cur, patch_size=patch_px)\n    else:\n        mask_bool = np.zeros((grid_side_local, grid_side_local), dtype=bool)\n        diff_vals = np.zeros((grid_side_local, grid_side_local), dtype=np.float32)\n\n    vis_changed = visualize_changed_patches(img_cur, mask_bool, patch_size=patch_px, alpha=0.25)\n    idx_i, idx_j = np.where(mask_bool)\n    overlay_rects = overlay_patch_rects(img_cur, list(zip(idx_i.tolist(), idx_j.tolist())), patch_size=patch_px, color=(255,0,0), thickness=2)\n\n    # Token delta grid: compare to previous frame full-encode for stable baseline\n    if idx > 0:\n        with torch.inference_mode():\n            feats_prev_full = state['encoder']._full_encode(frames_tensor[idx-1])\n        flat_prev = feats_prev_full.view(-1, feats_prev_full.shape[-1]).detach().float().cpu()\n    else:\n        flat_prev = feats_cur.view(-1, feats_cur.shape[-1]).detach().float().cpu()\n    flat_cur = feats_cur.view(-1, feats_cur.shape[-1]).detach().float().cpu()\n    delta = torch.linalg.norm(flat_cur - flat_prev, dim=-1)\n    delta_grid = delta.view(grid_side_local, grid_side_local).numpy()\n\n    # Attention\n    with torch.inference_mode():\n        img_feats = model.encode_images(frames_tensor[idx])\n        v_start, v_end, N = get_vision_span(input_ids, img_feats)\n        q_last = int((input_ids != tokenizer.pad_token_id).sum(dim=1).item()) - 1\n        q_idx = max(0, q_last - int(q_offset.value))\n        model_out = model(\n            input_ids=input_ids,\n            images=frames_tensor[idx],\n            image_sizes=[frames_pil[idx].size],\n            output_attentions=True,\n            return_dict=True\n        )\n        atts = model_out.attentions\n        B,H,Q,K = atts[-1].shape\n        att_head_idx.max = max(0, H-1)\n        att_vec = fuse_attention(\n            atts, q_idx, v_start, v_end,\n            layer_mode=att_layer_mode.value,\n            head_mode=('index' if att_head_mode.value == 'index' else 'mean'),\n            head_idx=int(att_head_idx.value)\n        ).detach().cpu().numpy()\n        att_grid = att_vec.reshape(grid_side_local, grid_side_local)\n\n    # Render\n    with output_widget:\n        clear_output(wait=True)\n        fig, ax = plt.subplots(2, 3, figsize=(16, 9))\n        ax[0,0].imshow(frames_pil[idx]); ax[0,0].set_title(f'Frame {idx}: {os.path.basename(frame_paths[idx])}'); ax[0,0].axis('off')\n        ax[0,1].imshow(vis_changed); ax[0,1].set_title(f'Changed patches (thr={float(diff_thr.value):.2f})'); ax[0,1].axis('off')\n        ax[0,2].imshow(overlay_rects); ax[0,2].set_title('Changed patch rectangles'); ax[0,2].axis('off')\n        im = ax[1,0].imshow(delta_grid, cmap='magma'); ax[1,0].set_title('Token Δ (L2 per patch)'); fig.colorbar(im, ax=ax[1,0], fraction=0.046, pad=0.04)\n        im2 = ax[1,1].imshow(att_grid, cmap='viridis'); ax[1,1].set_title(f'Attention (q={q_idx}, layer={att_layer_mode.value}, head={att_head_mode.value}{'' if att_head_mode.value=='mean' else f'[{int(att_head_idx.value)}]'} )'); fig.colorbar(im2, ax=ax[1,1], fraction=0.046, pad=0.04)\n        ax[1,2].axis('off')\n        text = (\n            f\"Decision: {info['encoding_type']} | Changed: {info['changed_patches']}/{info['total_patches']}\\n\"\n            f\"Context radius: {int(ctx_radius.value)} | Patch(px): {patch_px} | Grid: {grid_side_local}x{grid_side_local}\\n\"\n            f\"Vision span: [{v_start},{v_end}) N={N} | Tower: {type(vt).__name__}\"\n        )\n        ax[1,2].text(0.02, 0.5, text, fontsize=11, va='center')\n        plt.tight_layout(); plt.show()\n\n# ---------- Callbacks ----------\ndef on_frame_change(change):\n    render(int(change['new']))\ndef on_param_change(change):\n    render(int(frame_slider.value))\ndef on_reset_clicked(btn):\n    reset_sequence()\n    render(int(frame_slider.value))\n\nframe_slider.observe(on_frame_change, names='value')\nfor w in (diff_thr, max_changed, ctx_radius, att_layer_mode, att_head_mode, att_head_idx, q_offset):\n    w.observe(on_param_change, names='value')\nreset_seq_btn.on_click(on_reset_clicked)\n\n# Initial render\nrender(int(frame_slider.value))\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc27fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}