{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Vision: Start-Button Explorer\n",
    "\n",
    "This variant adds a Start button so you can:\n",
    "- Defer model loading until configuration is set\n",
    "- Initialize the interactive slider + attention heatmap UI on demand\n",
    "- Use the built-in Play control to watch frames over time (adjust interval)\n",
    "\n",
    "Configure paths below and click Start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, math\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nimport ipywidgets as W\nfrom IPython.display import display, clear_output\n\nfrom llava.model.builder import load_pretrained_model\nfrom llava.mm_utils import tokenizer_image_token, process_images\nfrom llava.constants import DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\nfrom llava.conversation import conv_templates\nfrom llava.utils import disable_torch_init\n\nfrom differential_vision import DifferentialVisionEncoder\nfrom differential_vision.patch_utils import (\n    compute_patch_diff, compute_patch_diff_values,\n    visualize_changed_patches, overlay_patch_rects\n)\n\n# --- Config UI ---\nmodel_path_txt = W.Text(value='/home/sdan/workspace/diffcua/checkpoints/llava-fastvithd_1.5b_stage3', description='Model', layout=W.Layout(width='60%'))\nbase_path_txt = W.Text(value='', description='Base', layout=W.Layout(width='40%'))\nimages_dir_txt = W.Text(value='/home/sdan/workspace/diffcua/differential_vision/agentnet_curated/traj_0000', description='Images', layout=W.Layout(width='60%'))\nprompt_txt = W.Textarea(value='Describe salient changes and where the text points its focus.', description='Prompt', layout=W.Layout(width='80%', height='60px'))\nforce_pad_chk = W.Checkbox(value=True, description='Force pad aspect')\nplay_interval = W.IntSlider(description='Interval(ms)', min=100, max=2000, step=50, value=600, continuous_update=False)\nstart_btn = W.Button(description='Start', button_style='success')\npanel = W.Output()\n\ndisplay(W.VBox([\n    W.HBox([model_path_txt, base_path_txt]),\n    images_dir_txt,\n    prompt_txt,\n    W.HBox([force_pad_chk, play_interval, start_btn]),\n    panel\n]))\n\ndef _natural_key(path: str):\n    import re\n    base = os.path.basename(path)\n    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', base)]\n\ndef _gather_images(directory: str):\n    supported = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n    files = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.splitext(f)[1].lower() in supported]\n    return sorted(files, key=_natural_key)\n\ndef on_start_clicked(_):\n    with panel:\n        clear_output(wait=True)\n        # Resolve config\n        device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n        torch_device = torch.device(device)\n        model_path = os.path.expanduser(model_path_txt.value.strip())\n        model_base = base_path_txt.value.strip() or None\n        images_dir = os.path.expanduser(images_dir_txt.value.strip())\n        prompt = prompt_txt.value.strip()\n        # Load model lazily\n        disable_torch_init()\n        try:\n            model_name = os.path.basename(model_path)\n            tokenizer, model, image_processor, _ = load_pretrained_model(model_path, model_base, model_name, device=device)\n            # Set attention implementation to eager to support output_attentions\n            model.set_attn_implementation('eager')\n        except Exception as e:\n            print('Error loading model:', e)\n            return\n        if force_pad_chk.value:\n            try: setattr(model.config, 'image_aspect_ratio', 'pad')\n            except Exception: pass\n        # Build prompt + input ids\n        def build_prompt_and_ids(user_prompt):\n            qs = user_prompt\n            if getattr(model.config, 'mm_use_im_start_end', False):\n                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n            else:\n                qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n            conv = conv_templates['qwen_2'].copy()\n            conv.append_message(conv.roles[0], qs)\n            conv.append_message(conv.roles[1], None)\n            text = conv.get_prompt()\n            ids = tokenizer_image_token(text, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(torch_device)\n            return text, ids\n        prompt_text, input_ids = build_prompt_and_ids(prompt)\n        # Gather images\n        try:\n            frame_paths = _gather_images(images_dir)\n        except Exception as e:\n            print('Error reading images:', e)\n            return\n        if not frame_paths:\n            print('No images found in', images_dir)\n            return\n        # Pre-load tensors\n        def load_and_process(path):\n            pil = Image.open(path).convert('RGB')\n            processed = process_images([pil], image_processor, model.config)\n            tensor = processed[0] if isinstance(processed, (list, tuple)) else processed\n            if tensor.ndim == 3: tensor = tensor.unsqueeze(0)\n            return pil, tensor.to(device=torch_device, dtype=next(model.parameters()).dtype)\n        frames_pil, frames_tensor = [], []\n        for p in frame_paths:\n            try:\n                pil, ten = load_and_process(p)\n            except Exception as e:\n                print('Error loading image', p, e); return\n            frames_pil.append(pil); frames_tensor.append(ten)\n        # Grid info\n        vt = model.get_vision_tower()\n        grid_side = getattr(vt, 'num_patches_per_side', None)\n        if grid_side is None:\n            with torch.inference_mode():\n                tmp_feats = model.encode_images(frames_tensor[0])\n            grid_side = int(round(math.sqrt(int(tmp_feats.shape[1]))))\n        grid_side = int(grid_side)\n        if hasattr(vt, 'config') and hasattr(vt.config, 'patch_size'):\n            patch_px_default = int(vt.config.patch_size)\n        elif isinstance(getattr(vt, 'config', None), dict):\n            patch_px_default = int(vt.config['image_cfg']['patch_size'])\n        else:\n            patch_px_default = 24\n        # Build encoder\n        def build_encoder(thr, maxchg, radius):\n            return DifferentialVisionEncoder(\n                model, diff_threshold=float(thr), max_changed_patches=int(maxchg), context_radius=int(radius), skip_small_updates=False, device=torch_device\n            )\n        encoder = build_encoder(0.05, 50, 1)\n        encoder.reset_cache()\n        # Widgets\n        frame_slider = W.IntSlider(description='Frame', min=0, max=len(frame_paths)-1, step=1, value=0, continuous_update=False)\n        play = W.Play(interval=int(play_interval.value), value=0, min=0, max=len(frame_paths)-1, step=1, description='Play')\n        W.jslink((play, 'value'), (frame_slider, 'value'))\n        diff_thr = W.FloatSlider(description='Diff thr', min=0.0, max=1.0, step=0.01, value=0.05, readout_format='.2f', continuous_update=False)\n        max_changed = W.IntSlider(description='Max Δ patches', min=1, max=grid_side*grid_side, step=1, value=50, continuous_update=False)\n        ctx_radius = W.IntSlider(description='Context r', min=0, max=3, step=1, value=1, continuous_update=False)\n        att_layer_mode = W.Dropdown(description='Layer', options=['last', 'mean_all'], value='last')\n        att_head_mode = W.Dropdown(description='Head', options=['mean', 'index'], value='mean')\n        att_head_idx = W.IntSlider(description='Head idx', min=0, max=15, step=1, value=0, continuous_update=False)\n        q_offset = W.IntSlider(description='Q offset', min=0, max=8, step=1, value=0, continuous_update=False)\n        out_vis = W.Output()\n        display(W.VBox([W.HBox([play, frame_slider]), W.HBox([diff_thr, max_changed, ctx_radius]), W.HBox([att_layer_mode, att_head_mode, att_head_idx, q_offset]), out_vis]))\n        # Helpers\n        def get_vision_span(ids, image_features):\n            N = int(image_features.shape[1])\n            pos = (ids[0] == IMAGE_TOKEN_INDEX).nonzero(as_tuple=False)\n            assert pos.numel() > 0\n            start = int(pos[0,0].item())\n            return start, start+N, N\n        def fuse_attention(atts, q_index, v_start, v_end, layer_mode='last', head_mode='mean', head_idx=0):\n            layers = [atts[-1]] if layer_mode=='last' else atts\n            vals = []\n            for A in layers:\n                A = A[0]  # [H,Q,K]\n                Aq = A[:, q_index, :]\n                if head_mode=='index':\n                    Aq = Aq[head_idx:head_idx+1,:].mean(dim=0)\n                else:\n                    Aq = Aq.mean(dim=0)\n                vals.append(Aq)\n            V = torch.stack(vals, dim=0).mean(dim=0)\n            return V[v_start:v_end]\n        # Render\n        def render(i):\n            nonlocal encoder\n            # Rebuild encoder if params changed\n            if (encoder.diff_threshold != float(diff_thr.value) or encoder.max_changed_patches != int(max_changed.value) or encoder.context_radius != int(ctx_radius.value)):\n                encoder = build_encoder(diff_thr.value, max_changed.value, ctx_radius.value)\n                encoder.reset_cache()\n                # warm through to i\n                for t in range(0, i+1):\n                    encoder.encode(frames_tensor[t], return_stats=False)\n            feats, info = encoder.encode(frames_tensor[i], return_stats=True)\n            # Mask\n            if hasattr(vt, 'config') and hasattr(vt.config, 'patch_size'): patch_px = int(vt.config.patch_size)\n            elif isinstance(getattr(vt, 'config', None), dict): patch_px = int(vt.config['image_cfg']['patch_size'])\n            else: patch_px = patch_px_default\n            img_cur = np.array(frames_pil[i]).astype(np.float32)/255.0\n            if i>0:\n                img_prev = np.array(frames_pil[i-1]).astype(np.float32)/255.0\n                mask_bool = compute_patch_diff(img_prev, img_cur, patch_size=patch_px, threshold=float(diff_thr.value))\n                diff_vals = compute_patch_diff_values(img_prev, img_cur, patch_size=patch_px)\n            else:\n                mask_bool = np.zeros((grid_side, grid_side), dtype=bool)\n                diff_vals = np.zeros((grid_side, grid_side), dtype=np.float32)\n            vis_changed = visualize_changed_patches(img_cur, mask_bool, patch_size=patch_px, alpha=0.25)\n            idx_i, idx_j = np.where(mask_bool)\n            overlay_rects = overlay_patch_rects(img_cur, list(zip(idx_i.tolist(), idx_j.tolist())), patch_size=patch_px, color=(255,0,0), thickness=2)\n            # Delta grid\n            if i>0:\n                with torch.inference_mode():\n                    prev_full = encoder._full_encode(frames_tensor[i-1])\n                flat_prev = prev_full.view(-1, prev_full.shape[-1]).detach().float().cpu()\n            else:\n                flat_prev = feats.view(-1, feats.shape[-1]).detach().float().cpu()\n            flat_cur = feats.view(-1, feats.shape[-1]).detach().float().cpu()\n            delta = torch.linalg.norm(flat_cur - flat_prev, dim=-1)\n            delta_grid = delta.view(grid_side, grid_side).numpy()\n            # Attention\n            with torch.inference_mode():\n                img_feats = model.encode_images(frames_tensor[i])\n                v_start, v_end, N = get_vision_span(input_ids, img_feats)\n                q_last = int((input_ids != tokenizer.pad_token_id).sum(dim=1).item()) - 1\n                q_idx = max(0, q_last - int(q_offset.value))\n                out = model(input_ids=input_ids, images=frames_tensor[i], image_sizes=[frames_pil[i].size], output_attentions=True, return_dict=True)\n                atts = out.attentions\n                B,H,Q,K = atts[-1].shape\n                att_head_idx.max = max(0, H-1)\n                att_vec = fuse_attention(atts, q_idx, v_start, v_end, layer_mode=att_layer_mode.value, head_mode=('index' if att_head_mode.value=='index' else 'mean'), head_idx=int(att_head_idx.value)).detach().cpu().numpy()\n                att_grid = att_vec.reshape(grid_side, grid_side)\n            # Draw\n            with out_vis:\n                clear_output(wait=True)\n                fig, ax = plt.subplots(2, 3, figsize=(16, 9))\n                ax[0,0].imshow(frames_pil[i]); ax[0,0].set_title(f'Frame {i}: {os.path.basename(frame_paths[i])}'); ax[0,0].axis('off')\n                ax[0,1].imshow(vis_changed); ax[0,1].set_title(f'Changed patches (thr={float(diff_thr.value):.2f})'); ax[0,1].axis('off')\n                ax[0,2].imshow(overlay_rects); ax[0,2].set_title('Changed patch rectangles'); ax[0,2].axis('off')\n                im = ax[1,0].imshow(delta_grid, cmap='magma'); ax[1,0].set_title('Token Δ (L2 per patch)'); fig.colorbar(im, ax=ax[1,0], fraction=0.046, pad=0.04)\n                im2 = ax[1,1].imshow(att_grid, cmap='viridis'); ax[1,1].set_title(f'Attention (layer={att_layer_mode.value}, head={att_head_mode.value}{'' if att_head_mode.value=='mean' else f'[{int(att_head_idx.value)}]'}, q={q_idx})'); fig.colorbar(im2, ax=ax[1,1], fraction=0.046, pad=0.04)\n                ax[1,2].axis('off')\n                text = (\n                    f\"Decision: {info['encoding_type']} | Changed: {info['changed_patches']}/{info['total_patches']}\\n\"\n                    f\"Context radius: {int(ctx_radius.value)} | Patch(px): {patch_px} | Grid: {grid_side}x{grid_side}\\n\"\n                    f\"Vision span: [{v_start},{v_end}) N={N} | Tower: {type(vt).__name__}\"\n                )\n                ax[1,2].text(0.02, 0.5, text, fontsize=11, va='center')\n                plt.tight_layout(); plt.show()\n        # Bind\n        def on_frame_change(change): render(int(change['new']))\n        def on_param_change(change): render(int(frame_slider.value))\n        frame_slider.observe(on_frame_change, names='value')\n        for w in (diff_thr, max_changed, ctx_radius, att_layer_mode, att_head_mode, att_head_idx, q_offset): w.observe(on_param_change, names='value')\n        # Initial\n        render(0)\n\nstart_btn.on_click(on_start_clicked)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf85f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}