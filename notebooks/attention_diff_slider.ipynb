{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Vision + Attention Explorer\n",
    "\n",
    "Play through an image sequence with a slider. For each frame, we:\n",
    "- Run differential vision encoding (cache/partial/full)\n",
    "- Prepare multimodal inputs and capture cross-attention\n",
    "- Visualize changed patches and attention over the vision grid\n",
    "\n",
    "Set model path and image directory, then run `run_explorer(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, inspect\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from differential_vision import DifferentialVisionEncoder\n",
    "from differential_vision.patch_utils import compute_patch_diff, visualize_changed_patches\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, expand2square\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.conversation import conv_templates\n",
    "from llava.constants import DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN, IMAGE_TOKEN_INDEX\n",
    "\n",
    "def _natural_sort_key(path: str):\n",
    "    basename = os.path.basename(path)\n",
    "    return [int(tok) if tok.isdigit() else tok.lower() for tok in re.split(r'(\\d+)', basename)]\n",
    "\n",
    "def _gather_images_from_dir(directory: str):\n",
    "    supported = {'.png', '.jpg', '.jpeg', '.bmp', '.gif'}\n",
    "    paths = []\n",
    "    for entry in os.listdir(directory):\n",
    "        full = os.path.join(directory, entry)\n",
    "        if os.path.isfile(full) and os.path.splitext(entry)[1].lower() in supported:\n",
    "            paths.append(full)\n",
    "    return sorted(paths, key=_natural_sort_key)\n",
    "\n",
    "def _prepare_prompt(tokenizer, model, prompt: str, conv_mode: str):\n",
    "    if getattr(model.config, 'mm_use_im_start_end', False):\n",
    "        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + prompt\n",
    "    else:\n",
    "        qs = DEFAULT_IMAGE_TOKEN + '\\n' + prompt\n",
    "    conv = conv_templates[conv_mode].copy()\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    return conv.get_prompt()\n",
    "\n",
    "def _compute_vision_span(input_ids: torch.Tensor, num_image_tokens: int):\n",
    "    # input_ids: [1, seq] with IMAGE_TOKEN_INDEX placeholders\n",
    "    ids = input_ids[0]\n",
    "    idx = (ids == IMAGE_TOKEN_INDEX).nonzero(as_tuple=False)\n",
    "    if idx.numel() == 0:\n",
    "        return None, None\n",
    "    start = int(idx[0].item())\n",
    "    return start, start + int(num_image_tokens)\n",
    "\n",
    "def _heat_to_overlay(image_np: np.ndarray, heat: np.ndarray, alpha: float = 0.35, cmap: str = 'jet') -> np.ndarray:\n",
    "    H, W = image_np.shape[:2]\n",
    "    gh, gw = heat.shape\n",
    "    cell_h = math.ceil(H / gh)\n",
    "    cell_w = math.ceil(W / gw)\n",
    "    # normalize heat to [0,1]\n",
    "    hmin = float(heat.min())\n",
    "    hmax = float(heat.max())\n",
    "    denom = (hmax - hmin) if (hmax - hmin) > 1e-6 else 1.0\n",
    "    heat_norm = (heat - hmin) / denom\n",
    "    heat_rgb_small = (cm.get_cmap(cmap)(heat_norm)[..., :3] * 255).astype(np.uint8)\n",
    "    up = np.kron(heat_rgb_small, np.ones((cell_h, cell_w, 1), dtype=np.uint8))\n",
    "    up = up[:H, :W]\n",
    "    blended = (image_np.astype(np.float32) * (1 - alpha) + up.astype(np.float32) * alpha).astype(np.uint8)\n",
    "    return blended\n",
    "\n",
    "def run_explorer(\n",
    "    model_path: str,\n",
    "    image_dir: str,\n",
    "    prompt_text: str = 'Describe the image.',\n",
    "    conv_mode: str = 'qwen_2',\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu'),\n",
    "    diff_threshold: float = 0.05,\n",
    "    diff_max_changed_patches: int = 50,\n",
    "    diff_skip_small: bool = False,\n",
    "):\n",
    "    # Load model\n",
    "    model_path = os.path.expanduser(model_path)\n",
    "    model_name = get_model_name_from_path(model_path)\n",
    "    tokenizer, model, image_processor, _ = load_pretrained_model(model_path, None, model_name, device=device)\n",
    "    # Force pad aspect to keep single-image inputs for differential updates\n",
    "    try:\n",
    "        setattr(model.config, 'image_aspect_ratio', 'pad')\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch_device = torch.device(device)\n",
    "    # Differential encoder wrapping\n",
    "    diff = DifferentialVisionEncoder(\n",
    "        model,\n",
    "        diff_threshold=diff_threshold,\n",
    "        max_changed_patches=diff_max_changed_patches,\n",
    "        skip_small_updates=diff_skip_small,\n",
    "        device=torch_device,\n",
    "    )\n",
    "    original_encode = getattr(model, 'encode_images', None)\n",
    "    def encode_images_with_cache(images, image_sizes=None, return_stats=False):\n",
    "        try:\n",
    "            return diff.encode(images, image_sizes=image_sizes, return_stats=return_stats)\n",
    "        except Exception:\n",
    "            if original_encode is not None:\n",
    "                try:\n",
    "                    return original_encode(images, image_sizes=image_sizes, return_stats=return_stats)\n",
    "                except TypeError:\n",
    "                    return original_encode(images)\n",
    "            raise\n",
    "    model.encode_images = encode_images_with_cache\n",
    "\n",
    "    # Gather frames\n",
    "    directory = os.path.expanduser(image_dir)\n",
    "    frames = _gather_images_from_dir(directory)\n",
    "    assert len(frames) > 0, f'No images found in {directory}'\n",
    "\n",
    "    # Grid/token info\n",
    "    vt = model.get_vision_tower()\n",
    "    grid_h = grid_w = int(vt.num_patches_per_side)\n",
    "    num_image_tokens = int(vt.num_patches)\n",
    "\n",
    "    # State caches\n",
    "    cache = {}  # idx -> dict with visuals and stats\n",
    "    last_computed = -1\n",
    "\n",
    "    # UI controls\n",
    "    play = widgets.Play(interval=600, value=0, min=0, max=len(frames)-1, step=1, description='Play', disabled=False)\n",
    "    slider = widgets.IntSlider(min=0, max=len(frames)-1, step=1, description='Frame', readout=True)\n",
    "    widgets.jslink((play, 'value'), (slider, 'value'))\n",
    "    reset_btn = widgets.Button(description='Reset', button_style='warning')\n",
    "    out = widgets.Output()\n",
    "\n",
    "    # Dtype for images\n",
    "    try:\n",
    "        model_dtype = next(model.parameters()).dtype\n",
    "    except StopIteration:\n",
    "        model_dtype = torch.float16 if torch_device.type != 'cpu' else torch.float32\n",
    "\n",
    "    def _process_frame(i: int):\n",
    "        # Load and preprocess\n",
    "        img = Image.open(frames[i]).convert('RGB')\n",
    "        processed = process_images([img], image_processor, model.config)\n",
    "        img_tensor = processed[0] if isinstance(processed, (list, tuple)) else processed\n",
    "        if img_tensor.ndim == 3:\n",
    "            img_tensor = img_tensor.unsqueeze(0)\n",
    "        img_tensor = img_tensor.to(device=torch_device, dtype=model_dtype)\n",
    "\n",
    "        # Run differential encode to update cache and get stats\n",
    "        features, info = diff.encode(img_tensor, image_sizes=[img.size], return_stats=True)\n",
    "\n",
    "        # Build prompt and input_ids\n",
    "        prompt = _prepare_prompt(tokenizer, model, prompt_text, conv_mode)\n",
    "        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(torch_device)\n",
    "\n",
    "        # Prepare multimodal inputs (this calls model.encode_images again; cheap due to cache hit)\n",
    "        (ii, position_ids, attention_mask, pkv, inputs_embeds, labels) = model.prepare_inputs_labels_for_multimodal(\n",
    "            input_ids, None, None, None, None, img_tensor, image_sizes=[img.size]\n",
    "        )\n",
    "\n",
    "        # Forward to capture attentions on the prompt embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model.forward(\n",
    "                input_ids=None,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=pkv,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                labels=None,\n",
    "                output_attentions=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "        # Aggregate attention: last layer, mean heads, query=last prompt token\n",
    "        if outputs.attentions is None or len(outputs.attentions) == 0:\n",
    "            attn_heat = None\n",
    "        else:\n",
    "            last = outputs.attentions[-1][0]  # (num_heads, seq, seq) for batch 0\n",
    "            head_mean = last.mean(dim=0)  # (seq, seq)\n",
    "            seq_len = head_mean.shape[0]\n",
    "            query_idx = seq_len - 1\n",
    "            vstart, vend = _compute_vision_span(ii, num_image_tokens)\n",
    "            if vstart is None:\n",
    "                attn_heat = None\n",
    "            else:\n",
    "                vweights = head_mean[query_idx, vstart:vend].detach().float().cpu().numpy()\n",
    "                attn_heat = vweights.reshape(grid_h, grid_w)\n",
    "\n",
    "        # Build display image identical to pad preprocessing for overlay\n",
    "        disp = expand2square(img, tuple(int(x*255) for x in image_processor.image_mean))\n",
    "        disp = disp.resize((image_processor.crop_size['width'], image_processor.crop_size['height']))\n",
    "        disp_np = np.array(disp)\n",
    "        # Changed mask for visualization (recomputed on display-res grid)\n",
    "        patch_size_px = disp_np.shape[0] // grid_h\n",
    "        if i == 0:\n",
    "            changed_mask = np.zeros((grid_h, grid_w), dtype=bool)\n",
    "        else:\n",
    "            prev_img = Image.open(frames[i-1]).convert('RGB')\n",
    "            prev_disp = expand2square(prev_img, tuple(int(x*255) for x in image_processor.image_mean))\n",
    "            prev_disp = prev_disp.resize((image_processor.crop_size['width'], image_processor.crop_size['height']))\n",
    "            prev_np = np.array(prev_disp)\n",
    "            changed_mask = compute_patch_diff(prev_np, disp_np, patch_size=patch_size_px, threshold=diff_threshold)\n",
    "\n",
    "        vis_changed = visualize_changed_patches(disp_np, changed_mask, patch_size=patch_size_px, alpha=0.35)\n",
    "        attn_overlay = _heat_to_overlay(disp_np, attn_heat, alpha=0.35) if attn_heat is not None else None\n",
    "\n",
    "        cache[i] = dict(\n",
    "            info=info,\n",
    "            disp=disp_np,\n",
    "            vis_changed=vis_changed,\n",
    "            attn_overlay=attn_overlay,\n",
    "        )\n",
    "\n",
    "    def _compute_to(target_idx: int):\n",
    "        nonlocal last_computed\n",
    "        for k in range(last_computed + 1, target_idx + 1):\n",
    "            _process_frame(k)\n",
    "            last_computed = k\n",
    "\n",
    "    def _on_reset(_):\n",
    "        nonlocal last_computed\n",
    "        diff.reset_cache()\n",
    "        cache.clear()\n",
    "        last_computed = -1\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            print('State reset. Move the slider to recompute.')\n",
    "\n",
    "    def _on_change(change):\n",
    "        with out:\n",
    "            clear_output(wait=True)\n",
    "            idx = int(change['new'])\n",
    "            if idx > last_computed:\n",
    "                _compute_to(idx)\n",
    "            item = cache[idx]\n",
    "            info = item['info']\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(11, 5))\n",
    "            axes[0].imshow(item['vis_changed'])\n",
    "            axes[0].set_title(f\"Changed patches â€” {info['encoding_type']} ({info['changed_patches']}/{info['total_patches']})\")\n",
    "            axes[0].axis('off')\n",
    "            if item['attn_overlay'] is not None:\n",
    "                axes[1].imshow(item['attn_overlay'])\n",
    "                axes[1].set_title('Cross-attention over vision tokens')\n",
    "            else:\n",
    "                axes[1].imshow(item['disp'])\n",
    "                axes[1].set_title('Attention unavailable')\n",
    "            axes[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print('Encoder stats:', diff.get_stats())\n",
    "\n",
    "    reset_btn.on_click(_on_reset)\n",
    "    slider.observe(_on_change, names='value')\n",
    "\n",
    "    display(widgets.HBox([play, slider, reset_btn]))\n",
    "    display(out)\n",
    "    # Bootstrap first frame\n",
    "    _compute_to(0)\n",
    "    slider.value = 0\n",
    "\n",
    "# Example usage (uncomment and set paths):\n",
    "# run_explorer(model_path='./checkpoints/your-llava', image_dir='./data/frames', prompt_text='What happened?', conv_mode='qwen_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edf343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d64450cc8e4075b90c919040c59c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_explorer(model_path='/home/sdan/storage/diffcua/checkpoints/llava-fastvithd_7b_stage3', image_dir='/home/sdan/storage/diffcua/differential_vision/agentnet_curated/traj_0000', prompt_text='What happened?', conv_mode='qwen_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cca463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
